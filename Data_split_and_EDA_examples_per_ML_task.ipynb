{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EladMoshe98/testrepo/blob/main/Data_split_and_EDA_examples_per_ML_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split approaches\n",
        "\n",
        "* **Regression:** Randomly split data since targets are continuous and assumed independent.\n",
        "* **Classification:** Use a stratified split, meaning each subset keeps the same class proportions as the full dataset (important for balanced evaluation).\n",
        "* **Anomaly Detection:** Train only on normal data and reserve anomalies for validation/testing.\n",
        "* **Clustering:** Randomly split (or use all data) since there are no labels to guide splitting.\n",
        "* **Forecasting:** Split chronologically to preserve time order and prevent future data leakage.\n",
        "\n"
      ],
      "metadata": {
        "id": "9fijSTcGaVgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets used for data split examples\n",
        "\n",
        "**California Housing (Regression):**\n",
        "Tabular dataset (~20,640 samples, 8 numeric features like median income, house age, population, latitude/longitude) predicting median house value; used for regression tasks.\n",
        "\n",
        "https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
        "\n",
        "---\n",
        "\n",
        "**Wine (Classification):**\n",
        "Tabular dataset (178 samples, 13 chemical features such as alcohol, ash, magnesium) classifying wines into 3 cultivars; used for multi-class classification.\n",
        "\n",
        "https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset\n",
        "\n",
        "---\n",
        "\n",
        "**Synthetic Anomaly Data (Anomaly Detection):**\n",
        "Artificial dataset (1000 samples × 10 features) with 1% labeled as anomalies; used to simulate rare-event detection scenarios like fraud or fault detection.\n",
        "\n",
        "https://scikit-learn.org/stable/datasets/sample_generators.html (conceptual reference for synthetic data generation)\n",
        "\n",
        "---\n",
        "\n",
        "**Blobs (Clustering):**\n",
        "Synthetic dataset (1000 samples, 5 numeric features) generated around 3 cluster centers; used for testing unsupervised clustering algorithms.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
        "\n",
        "---\n",
        "\n",
        "**Synthetic Time Series (Forecasting):**\n",
        "Simulated dataset (1000 daily records with date and noisy sine value column) representing temporal data for time series forecasting examples.\n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.date_range.html (used to generate time series dates)\n"
      ],
      "metadata": {
        "id": "E3mD2wgFcn71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split for Regression\n",
        "\n",
        "- Regression tasks predict continuous values.\n",
        "- Stratification isn't possible (no discrete labels).\n",
        "- Split randomly (assuming IID data).\n",
        "- Consider k-fold CV instead of fixed validation if dataset is small."
      ],
      "metadata": {
        "id": "EvhklxgTZux7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU40d0UlZmOR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load example regression dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Typical split: 70% train, 15% validation, 15% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split for Classification\n",
        "\n",
        "- Always use `stratify=y` for classification splits to preserve label distribution.\n",
        "- Prevents bias when one class is underrepresented.\n",
        "- For small datasets, use StratifiedKFold or StratifiedShuffleSplit."
      ],
      "metadata": {
        "id": "6a6rWpAFZ9Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Stratified split ensures class balance in all sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "dvKYWho6ZnMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split for Anomaly Detection\n",
        "\n",
        "- Training data: ONLY normal samples (unsupervised anomaly detection).\n",
        "- Validation/Test: Mix normal + anomaly for evaluation.\n",
        "- Don't use stratified split—imbalanced by design.\n",
        "- Ensure anomalies don’t leak into training.\n",
        "\n"
      ],
      "metadata": {
        "id": "bF1cdDGBZ9ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example: 99% normal, 1% anomalies\n",
        "n_samples = 1000\n",
        "X = np.random.randn(n_samples, 10)\n",
        "y = np.zeros(n_samples)\n",
        "y[:10] = 1  # mark 1% anomalies\n",
        "\n",
        "# Split so anomalies are *only* in validation/test ideally\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X[y == 0], y[y == 0], test_size=0.3, random_state=42\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Add anomalies to val/test sets\n",
        "X_val = np.vstack([X_val, X[y == 1][:5]])\n",
        "y_val = np.concatenate([y_val, y[y == 1][:5]])\n",
        "X_test = np.vstack([X_test, X[y == 1][5:]])\n",
        "y_test = np.concatenate([y_test, y[y == 1][5:]])"
      ],
      "metadata": {
        "id": "JrOO5xQWZnEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split for Clustering\n",
        "\n",
        "- No y labels → unsupervised.\n",
        "- Often, all data is used for training (since we want full structure).\n",
        "- Use a hold-out set to test cluster stability/generalization.\n",
        "- Alternative: Cross-validation via repeated sub-sampling (e.g. evaluating clustering consistency)."
      ],
      "metadata": {
        "id": "gs4E0ATfZ9vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, _ = make_blobs(n_samples=1000, centers=3, n_features=5, random_state=42)\n",
        "\n",
        "# Random split since there are no labels\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "L-tsj_B3Zm69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data split for Forecasting\n",
        "\n",
        "- NEVER shuffle time series — order matters.\n",
        "- Train/Val/Test split must preserve temporal order.\n",
        "- Alternative: Use expanding-window or rolling-window cross-validation.\n",
        "- Validation simulates “future unseen” predictions."
      ],
      "metadata": {
        "id": "rMzRlzm7dRPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated time series data\n",
        "dates = pd.date_range(start=\"2020-01-01\", periods=1000, freq=\"D\")\n",
        "data = pd.DataFrame({\"date\": dates, \"value\": np.sin(np.arange(1000) / 50) + np.random.randn(1000)*0.1})\n",
        "\n",
        "# Split chronologically — no random shuffle\n",
        "train_size = int(len(data) * 0.7)\n",
        "val_size = int(len(data) * 0.15)\n",
        "\n",
        "train = data.iloc[:train_size]\n",
        "val = data.iloc[train_size:train_size + val_size]\n",
        "test = data.iloc[train_size + val_size:]"
      ],
      "metadata": {
        "id": "zg6UXpqDdRq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Unified EDA Summary for All Five ML Task Types\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing, load_wine, make_blobs\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n",
        "\n",
        "def print_section(title):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"📘 {title}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "def summarize_split(name, X_train, X_val=None, X_test=None, y_train=None, y_val=None, y_test=None):\n",
        "    \"\"\"Summarizes numeric stats across full dataset vs splits.\"\"\"\n",
        "    def summary(arr):\n",
        "        if arr is None or len(arr) == 0: # Handle empty arrays\n",
        "            return {\"mean\": np.nan, \"std\": np.nan, \"shape\": (0,) if arr is None else arr.shape}\n",
        "        arr = np.array(arr)\n",
        "        return {\n",
        "            \"mean\": np.mean(arr, axis=0)[:3] if arr.ndim > 1 else np.mean(arr),\n",
        "            \"std\": np.std(arr, axis=0)[:3] if arr.ndim > 1 else np.std(arr),\n",
        "            \"shape\": arr.shape\n",
        "        }\n",
        "\n",
        "    print(f\"\\n{name} Split Summary:\")\n",
        "    print(f\"Train size: {len(X_train)}\")\n",
        "    if X_val is not None:\n",
        "        print(f\"Val size: {len(X_val)}\")\n",
        "    if X_test is not None:\n",
        "        print(f\"Test size: {len(X_test)}\")\n",
        "\n",
        "    all_X = [X_train]\n",
        "    if X_val is not None and len(X_val) > 0:\n",
        "        all_X.append(X_val)\n",
        "    if X_test is not None and len(X_test) > 0:\n",
        "        all_X.append(X_test)\n",
        "    full_X = np.vstack(all_X)\n",
        "\n",
        "\n",
        "    print(\"Feature mean (first 3 dims):\")\n",
        "    print(f\"  Full:  {summary(full_X)['mean']}\")\n",
        "    print(f\"  Train: {summary(X_train)['mean']}\")\n",
        "    if X_val is not None:\n",
        "        print(f\"  Val:   {summary(X_val)['mean']}\")\n",
        "    if X_test is not None:\n",
        "        print(f\"  Test:  {summary(X_test)['mean']}\")\n",
        "\n",
        "\n",
        "    if y_train is not None:\n",
        "        all_y = [y_train]\n",
        "        if y_val is not None and len(y_val) > 0:\n",
        "             all_y.append(y_val)\n",
        "        if y_test is not None and len(y_test) > 0:\n",
        "             all_y.append(y_test)\n",
        "        full_y = np.concatenate(all_y)\n",
        "        print(\"Target mean/std:\")\n",
        "        print(f\"  Full:  mean={np.mean(full_y):.3f}, std={np.std(full_y):.3f}\")\n",
        "        print(f\"  Train: mean={np.mean(y_train):.3f}, std={np.std(y_train):.3f}\")\n",
        "        if y_val is not None:\n",
        "             print(f\"  Val:   mean={np.mean(y_val):.3f}, std={np.std(y_val):.3f}\")\n",
        "        if y_test is not None:\n",
        "             print(f\"  Test:  mean={np.mean(y_test):.3f}, std={np.std(y_test):.3f}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Regression\n",
        "# ================================================================\n",
        "print_section(\"Regression — California Housing\")\n",
        "# Load the California Housing data again to get the correct X and feature names\n",
        "X_cal, y_cal = fetch_california_housing(return_X_y=True)\n",
        "print(pd.DataFrame(X_cal, columns=fetch_california_housing().feature_names).describe().T)\n",
        "# Use the split variables from the regression section (lU40d0UlZmOR)\n",
        "X_train_reg, X_temp_reg, y_train_reg, y_temp_reg = train_test_split(X_cal, y_cal, test_size=0.3, random_state=42)\n",
        "X_val_reg, X_test_reg, y_val_reg, y_test_reg = train_test_split(X_temp_reg, y_temp_reg, test_size=0.5, random_state=42)\n",
        "summarize_split(\"California Housing\", X_train_reg, X_val_reg, X_test_reg, y_train_reg, y_val_reg, y_test_reg)\n",
        "\n",
        "# ================================================================\n",
        "# Classification\n",
        "# ================================================================\n",
        "print_section(\"Classification — Wine Dataset\")\n",
        "# Load the Wine data again to get the correct X and y\n",
        "X_wine, y_wine = load_wine(return_X_y=True)\n",
        "print(\"Class distribution (full):\", dict(zip(*np.unique(y_wine, return_counts=True))))\n",
        "print(pd.DataFrame(X_wine, columns=load_wine().feature_names).describe().T)\n",
        "# Use the split variables from the classification section (dvKYWho6ZnMC)\n",
        "X_train_clf, X_temp_clf, y_train_clf, y_temp_clf = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.3, stratify=y_wine, random_state=42\n",
        ")\n",
        "X_val_clf, X_test_clf, y_val_clf, y_test_clf = train_test_split(\n",
        "    X_temp_clf, y_temp_clf, test_size=0.5, stratify=y_temp_clf, random_state=42\n",
        ")\n",
        "summarize_split(\"Wine Classification\", X_train_clf, X_val_clf, X_test_clf, y_train_clf, y_val_clf, y_test_clf)\n",
        "\n",
        "# ================================================================\n",
        "# Anomaly Detection\n",
        "# ================================================================\n",
        "print_section(\"Anomaly Detection — Synthetic\")\n",
        "# Use the X from the anomaly detection section (JrOO5xQWZnEF)\n",
        "n_samples = 1000\n",
        "X_ano = np.random.randn(n_samples, 10)\n",
        "y_ano = np.zeros(n_samples)\n",
        "y_ano[:10] = 1  # mark 1% anomalies\n",
        "print(\"Overall Feature Summary (first 3 features):\")\n",
        "print(pd.DataFrame(X_ano[:, :3], columns=[\"f1\", \"f2\", \"f3\"]).describe().T)\n",
        "# Use the y from the anomaly detection section (JrOO5xQWZnEF)\n",
        "print(\"Anomaly ratio full:\", np.mean(y_ano))\n",
        "# Use the split variables from the anomaly detection section (JrOO5xQWZnEF)\n",
        "X_train_ano, X_test_ano, y_train_ano, y_test_ano = train_test_split(\n",
        "    X_ano, y_ano, test_size=0.2, random_state=42, stratify=y_ano # Stratify to ensure anomalies are in test set\n",
        ")\n",
        "summarize_split(\"Anomaly Detection\", X_train_ano, X_test=X_test_ano, y_train=y_train_ano, y_test=y_test_ano)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Clustering\n",
        "# ================================================================\n",
        "print_section(\"Clustering — Synthetic Blobs\")\n",
        "# Load the Blobs data again to get the correct X\n",
        "X_blobs, _ = make_blobs(n_samples=1000, centers=3, n_features=5, random_state=42)\n",
        "print(pd.DataFrame(X_blobs[:, :3], columns=[\"f1\", \"f2\", \"f3\"]).describe().T)\n",
        "# Use the split variables from the clustering section (L-tsj_B3Zm69)\n",
        "X_train_clus, X_test_clus = train_test_split(X_blobs, test_size=0.2, random_state=42)\n",
        "summarize_split(\"Clustering\", X_train_clus, X_test=X_test_clus) # Corrected X_test and X_val\n",
        "\n",
        "# ================================================================\n",
        "# Forecasting\n",
        "# ================================================================\n",
        "print_section(\"Forecasting — Synthetic Time Series\")\n",
        "# Use the data from the forecasting section (zg6UXpqDdRq_)\n",
        "dates = pd.date_range(start=\"2020-01-01\", periods=1000, freq=\"D\")\n",
        "data_ts = pd.DataFrame({\"date\": dates, \"value\": np.sin(np.arange(1000) / 50) + np.random.randn(1000)*0.1})\n",
        "train_size_ts = int(len(data_ts) * 0.7)\n",
        "val_size_ts = int(len(data_ts) * 0.15)\n",
        "\n",
        "train_ts = data_ts.iloc[:train_size_ts]\n",
        "val_ts = data_ts.iloc[train_size_ts:train_size_ts + val_size_ts]\n",
        "test_ts = data_ts.iloc[train_size_ts + val_size_ts:]\n",
        "\n",
        "print(data_ts.describe().T)\n",
        "print(\"\\nSplit Sizes:\")\n",
        "print(f\"Train: {len(train_ts)} | Val: {len(val_ts)} | Test: {len(test_ts)}\")\n",
        "print(\"Mean/std per split:\")\n",
        "print(f\"  Train: mean={train_ts['value'].mean():.3f}, std={train_ts['value'].std():.3f}\")\n",
        "print(f\"  Val:   mean={val_ts['value'].mean():.3f}, std={val_ts['value'].std():.3f}\")\n",
        "print(f\"  Test:  mean={test_ts['value'].mean():.3f}, std={test_ts['value'].std():.3f}\")\n",
        "\n",
        "# ================================================================\n",
        "# Done\n",
        "# ================================================================\n",
        "print_section(\"All EDA Comparisons Completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyN5WmrWfTbZ",
        "outputId": "20634351-94a6-44d4-c1f6-0bd51c81352a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "📘 Regression — California Housing\n",
            "======================================================================\n",
            "                count      mean       std      min      25%       50%  \\\n",
            "MedInc     20,640.000     3.871     1.900    0.500    2.563     3.535   \n",
            "HouseAge   20,640.000    28.639    12.586    1.000   18.000    29.000   \n",
            "AveRooms   20,640.000     5.429     2.474    0.846    4.441     5.229   \n",
            "AveBedrms  20,640.000     1.097     0.474    0.333    1.006     1.049   \n",
            "Population 20,640.000 1,425.477 1,132.462    3.000  787.000 1,166.000   \n",
            "AveOccup   20,640.000     3.071    10.386    0.692    2.430     2.818   \n",
            "Latitude   20,640.000    35.632     2.136   32.540   33.930    34.260   \n",
            "Longitude  20,640.000  -119.570     2.004 -124.350 -121.800  -118.490   \n",
            "\n",
            "                 75%        max  \n",
            "MedInc         4.743     15.000  \n",
            "HouseAge      37.000     52.000  \n",
            "AveRooms       6.052    141.909  \n",
            "AveBedrms      1.100     34.067  \n",
            "Population 1,725.000 35,682.000  \n",
            "AveOccup       3.282  1,243.333  \n",
            "Latitude      37.710     41.950  \n",
            "Longitude   -118.010   -114.310  \n",
            "\n",
            "California Housing Split Summary:\n",
            "Train size: 14448\n",
            "Val size: 3096\n",
            "Test size: 3096\n",
            "Feature mean (first 3 dims):\n",
            "  Full:  [ 3.870671   28.63948643  5.42899974]\n",
            "  Train: [ 3.87689155 28.57537375  5.43812463]\n",
            "  Val:   [ 3.8360157  28.78165375  5.35630856]\n",
            "  Test:  [ 3.87629709 28.79651163  5.45910811]\n",
            "Target mean/std:\n",
            "  Full:  mean=2.069, std=1.154\n",
            "  Train: mean=2.069, std=1.157\n",
            "  Val:   mean=2.068, std=1.141\n",
            "  Test:  mean=2.066, std=1.150\n",
            "\n",
            "======================================================================\n",
            "📘 Classification — Wine Dataset\n",
            "======================================================================\n",
            "Class distribution (full): {np.int64(0): np.int64(59), np.int64(1): np.int64(71), np.int64(2): np.int64(48)}\n",
            "                               count    mean     std     min     25%     50%  \\\n",
            "alcohol                      178.000  13.001   0.812  11.030  12.362  13.050   \n",
            "malic_acid                   178.000   2.336   1.117   0.740   1.603   1.865   \n",
            "ash                          178.000   2.367   0.274   1.360   2.210   2.360   \n",
            "alcalinity_of_ash            178.000  19.495   3.340  10.600  17.200  19.500   \n",
            "magnesium                    178.000  99.742  14.282  70.000  88.000  98.000   \n",
            "total_phenols                178.000   2.295   0.626   0.980   1.742   2.355   \n",
            "flavanoids                   178.000   2.029   0.999   0.340   1.205   2.135   \n",
            "nonflavanoid_phenols         178.000   0.362   0.124   0.130   0.270   0.340   \n",
            "proanthocyanins              178.000   1.591   0.572   0.410   1.250   1.555   \n",
            "color_intensity              178.000   5.058   2.318   1.280   3.220   4.690   \n",
            "hue                          178.000   0.957   0.229   0.480   0.782   0.965   \n",
            "od280/od315_of_diluted_wines 178.000   2.612   0.710   1.270   1.938   2.780   \n",
            "proline                      178.000 746.893 314.907 278.000 500.500 673.500   \n",
            "\n",
            "                                 75%       max  \n",
            "alcohol                       13.678    14.830  \n",
            "malic_acid                     3.083     5.800  \n",
            "ash                            2.558     3.230  \n",
            "alcalinity_of_ash             21.500    30.000  \n",
            "magnesium                    107.000   162.000  \n",
            "total_phenols                  2.800     3.880  \n",
            "flavanoids                     2.875     5.080  \n",
            "nonflavanoid_phenols           0.438     0.660  \n",
            "proanthocyanins                1.950     3.580  \n",
            "color_intensity                6.200    13.000  \n",
            "hue                            1.120     1.710  \n",
            "od280/od315_of_diluted_wines   3.170     4.000  \n",
            "proline                      985.000 1,680.000  \n",
            "\n",
            "Wine Classification Split Summary:\n",
            "Train size: 124\n",
            "Val size: 27\n",
            "Test size: 27\n",
            "Feature mean (first 3 dims):\n",
            "  Full:  [13.00061798  2.33634831  2.36651685]\n",
            "  Train: [12.9558871   2.28701613  2.36346774]\n",
            "  Val:   [13.26037037  2.50518519  2.33555556]\n",
            "  Test:  [12.9462963   2.39407407  2.41148148]\n",
            "Target mean/std:\n",
            "  Full:  mean=0.938, std=0.773\n",
            "  Train: mean=0.935, std=0.770\n",
            "  Val:   mean=0.963, std=0.793\n",
            "  Test:  mean=0.926, std=0.766\n",
            "\n",
            "======================================================================\n",
            "📘 Anomaly Detection — Synthetic\n",
            "======================================================================\n",
            "Overall Feature Summary (first 3 features):\n",
            "       count   mean   std    min    25%    50%   75%   max\n",
            "f1 1,000.000 -0.001 0.964 -3.628 -0.645  0.059 0.648 3.005\n",
            "f2 1,000.000 -0.015 0.981 -2.926 -0.659 -0.010 0.652 3.009\n",
            "f3 1,000.000 -0.010 1.015 -3.368 -0.680 -0.045 0.666 4.327\n",
            "Anomaly ratio full: 0.01\n",
            "\n",
            "Anomaly Detection Split Summary:\n",
            "Train size: 800\n",
            "Test size: 200\n",
            "Feature mean (first 3 dims):\n",
            "  Full:  [-0.00084379 -0.01475355 -0.01025155]\n",
            "  Train: [-0.00502026 -0.00879303 -0.02698897]\n",
            "  Test:  [ 0.01586213 -0.03859561  0.05669817]\n",
            "Target mean/std:\n",
            "  Full:  mean=0.010, std=0.099\n",
            "  Train: mean=0.010, std=0.099\n",
            "  Test:  mean=0.010, std=0.099\n",
            "\n",
            "======================================================================\n",
            "📘 Clustering — Synthetic Blobs\n",
            "======================================================================\n",
            "       count   mean   std     min    25%    50%    75%    max\n",
            "f1 1,000.000 -6.318 3.076 -12.215 -8.985 -6.942 -3.153  0.863\n",
            "f2 1,000.000  3.214 8.593 -11.258 -8.092  8.595  9.543 11.968\n",
            "f3 1,000.000  6.191 1.515   1.866  5.029  6.260  7.299 10.039\n",
            "\n",
            "Clustering Split Summary:\n",
            "Train size: 800\n",
            "Test size: 200\n",
            "Feature mean (first 3 dims):\n",
            "  Full:  [-6.3183032   3.21362476  6.19076345]\n",
            "  Train: [-6.35089283  3.40462613  6.21134132]\n",
            "  Test:  [-6.18794467  2.44961928  6.10845196]\n",
            "\n",
            "======================================================================\n",
            "📘 Forecasting — Synthetic Time Series\n",
            "======================================================================\n",
            "          count                 mean                  min  \\\n",
            "date       1000  2021-05-14 12:00:00  2020-01-01 00:00:00   \n",
            "value 1,000.000                0.033               -1.232   \n",
            "\n",
            "                       25%                  50%                  75%  \\\n",
            "date   2020-09-06 18:00:00  2021-05-14 12:00:00  2022-01-19 06:00:00   \n",
            "value               -0.664                0.069                0.718   \n",
            "\n",
            "                       max   std  \n",
            "date   2022-09-26 00:00:00   NaN  \n",
            "value                1.189 0.709  \n",
            "\n",
            "Split Sizes:\n",
            "Train: 700 | Val: 150 | Test: 150\n",
            "Mean/std per split:\n",
            "  Train: mean=0.061, std=0.711\n",
            "  Val:   mean=0.152, std=0.689\n",
            "  Test:  mean=-0.216, std=0.670\n",
            "\n",
            "======================================================================\n",
            "📘 All EDA Comparisons Completed\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}